{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64440991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn,save,load\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bb050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:10<00:00, 938849.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 66547.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 911141.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2357155.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root=\"data\", download=True, train=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"data\", download=True, train=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e84ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image classifier model\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 22 * 22, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d90350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the image classifier model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classifier = ImageClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ac1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(classifier.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f62db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    classifier.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = classifier(images)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    mlflow.log_metric(\"train_loss\", loss.item())\n",
    "    print(f\"Epoch:{epoch} loss is {loss.item()}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b9fec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on validation set should also be calculated.\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "test_counters = []\n",
    "\n",
    "def test():\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = classifier(images)\n",
    "            test_loss += loss_fn(output, labels).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "393b8d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 loss is 4.097577857464785e-06\n",
      "Epoch:1 loss is 4.842875256372281e-08\n",
      "Epoch:2 loss is 8.64255923715973e-07\n",
      "Epoch:3 loss is 0.0\n",
      "Epoch:4 loss is 0.0\n",
      "Epoch:5 loss is 0.0\n",
      "Epoch:6 loss is 2.86846074004643e-07\n",
      "Epoch:7 loss is 1.1175869119028903e-08\n",
      "Epoch:8 loss is 0.000425531470682472\n",
      "Epoch:9 loss is 0.0\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 9881/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"/mnt/c/Users/HP/Assign/mlruns\")\n",
    "\n",
    "# Auto-logging\n",
    "mlflow.set_experiment(experiment_name=\"MNIST Classifier\")\n",
    "with mlflow.start_run():\n",
    "    # Train the model\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        train(epoch)\n",
    "\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6dfc0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ddae94a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdfa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(classifier.state_dict(), 'model_state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "with open('model_state.pt', 'rb') as f: \n",
    "     classifier.load_state_dict(load(f))  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7aa39942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on an image\n",
    "classifier.eval()\n",
    "img = Image.open('data/test_images/nine.png')\n",
    "img = img.resize((28,28))\n",
    "img_transform = transforms.Compose([transforms.ToTensor()])\n",
    "img_tensor = img_transform(img).unsqueeze(0).to(device)\n",
    "output = classifier(img_tensor)\n",
    "predicted_label = torch.argmax(output)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cad86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
